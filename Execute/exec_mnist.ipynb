{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from QF_FB_C.lib_mlp import *\n",
    "from QF_FB_C.lib_qf_fb import *\n",
    "from QF_Net.lib_qf_net import *\n",
    "from QF_Net.lib_util import *\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimizer, device, epoch,interest_num,criterion,train_loader):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target, new_target = modify_target(target,interest_num)\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, True)\n",
    "\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 50 == 0:            \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}/{} ({:.2f}%)'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss, correct, (batch_idx + 1) * len(data),\n",
    "                       100. * float(correct) / float(((batch_idx + 1) * len(data)))))\n",
    "    print(\"-\" * 20, \"training done, loss\", \"-\" * 20)    \n",
    "\n",
    "def test(model,device,interest_num,criterion,test_loader):\n",
    "    accur = []\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target, new_target = modify_target(target,interest_num)\n",
    "        data, target = data.to(device), target.to(device)        \n",
    "        output = model(data, False)        \n",
    "        test_loss += criterion(output, target)  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    a = 100. * correct / len(test_loader.dataset)\n",
    "    accur.append(a)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / float(len(test_loader.dataset))))\n",
    "\n",
    "    return float(correct) / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def load_data(interest_num,img_size,batch_size,inference_batch_size,num_workers):\n",
    "    # convert data to torch.FloatTensor\n",
    "    transform = transforms.Compose([transforms.Resize((img_size, img_size)), transforms.ToTensor()])    \n",
    "    train_data = datasets.MNIST(root='data', train=True,\n",
    "                                download=True, transform=transform)\n",
    "    test_data = datasets.MNIST(root='data', train=False,\n",
    "                               download=True, transform=transform)\n",
    "    train_data = select_num(train_data, interest_num)\n",
    "    test_data = select_num(test_data, interest_num)\n",
    "\n",
    "    # prepare data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size,\n",
    "                                              num_workers=num_workers, shuffle=False, drop_last=True)\n",
    "\n",
    "    return train_loader,test_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Training procedure for Quantum Computer:\n",
      "\tStart at: 07/02/2020 08:49:57\n",
      "\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\n",
      "\tEnjoy and Good Luck!\n",
      "====================================================================================================\n",
      "\n",
      "==================== 0 epoch ====================\n",
      "Epoch Start at: 07/02/2020 08:49:58\n",
      "-------------------- learning rates --------------------\n",
      "0.1,0.1,0.1,0.1,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 07/02/2020 08:49:58\n",
      "Train Epoch: 0 [0/12049 (0%)]\tLoss: 0.692409\tAccuracy: 22/32 (68.75%)\n",
      "Train Epoch: 0 [1600/12049 (13%)]\tLoss: 0.671228\tAccuracy: 1530/1632 (93.75%)\n",
      "Train Epoch: 0 [3200/12049 (27%)]\tLoss: 0.671653\tAccuracy: 3077/3232 (95.20%)\n",
      "Train Epoch: 0 [4800/12049 (40%)]\tLoss: 0.673316\tAccuracy: 4616/4832 (95.53%)\n",
      "Train Epoch: 0 [6400/12049 (53%)]\tLoss: 0.669353\tAccuracy: 6141/6432 (95.48%)\n",
      "Train Epoch: 0 [8000/12049 (66%)]\tLoss: 0.669420\tAccuracy: 7670/8032 (95.49%)\n",
      "Train Epoch: 0 [9600/12049 (80%)]\tLoss: 0.677605\tAccuracy: 9199/9632 (95.50%)\n",
      "Train Epoch: 0 [11200/12049 (93%)]\tLoss: 0.666153\tAccuracy: 10733/11232 (95.56%)\n",
      "-------------------- training done, loss --------------------\n",
      "Trainign End at: 07/02/2020 08:50:08\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 07/02/2020 08:50:08\n",
      "Test set: Average loss: 0.0207, Accuracy: 1883/1968 (95.68%)\n",
      "Testing End at: 07/02/2020 08:50:09\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9568089430894309; Current accuracy 0.9568089430894309. Checkpointing\n",
      "Epoch End at: 07/02/2020 08:50:09\n",
      "============================================================\n",
      "\n",
      "==================== 1 epoch ====================\n",
      "Epoch Start at: 07/02/2020 08:50:09\n",
      "-------------------- learning rates --------------------\n",
      "0.010000000000000002,0.010000000000000002,0.010000000000000002,0.010000000000000002,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 07/02/2020 08:50:09\n",
      "Train Epoch: 1 [0/12049 (0%)]\tLoss: 0.673631\tAccuracy: 30/32 (93.75%)\n",
      "Train Epoch: 1 [1600/12049 (13%)]\tLoss: 0.667995\tAccuracy: 1570/1632 (96.20%)\n",
      "Train Epoch: 1 [3200/12049 (27%)]\tLoss: 0.669594\tAccuracy: 3114/3232 (96.35%)\n",
      "Train Epoch: 1 [4800/12049 (40%)]\tLoss: 0.669415\tAccuracy: 4663/4832 (96.50%)\n",
      "Train Epoch: 1 [6400/12049 (53%)]\tLoss: 0.671196\tAccuracy: 6207/6432 (96.50%)\n",
      "Train Epoch: 1 [8000/12049 (66%)]\tLoss: 0.670984\tAccuracy: 7734/8032 (96.29%)\n",
      "Train Epoch: 1 [9600/12049 (80%)]\tLoss: 0.671099\tAccuracy: 9272/9632 (96.26%)\n",
      "Train Epoch: 1 [11200/12049 (93%)]\tLoss: 0.668966\tAccuracy: 10805/11232 (96.20%)\n",
      "-------------------- training done, loss --------------------\n",
      "Trainign End at: 07/02/2020 08:50:19\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 07/02/2020 08:50:19\n",
      "Test set: Average loss: 0.0207, Accuracy: 1882/1968 (95.63%)\n",
      "Testing End at: 07/02/2020 08:50:20\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9568089430894309; Current accuracy 0.9563008130081301. Checkpointing\n",
      "Epoch End at: 07/02/2020 08:50:20\n",
      "============================================================\n",
      "\n",
      "==================== 2 epoch ====================\n",
      "Epoch Start at: 07/02/2020 08:50:20\n",
      "-------------------- learning rates --------------------\n",
      "0.010000000000000002,0.010000000000000002,0.010000000000000002,0.010000000000000002,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 07/02/2020 08:50:20\n",
      "Train Epoch: 2 [0/12049 (0%)]\tLoss: 0.671825\tAccuracy: 30/32 (93.75%)\n",
      "Train Epoch: 2 [1600/12049 (13%)]\tLoss: 0.670105\tAccuracy: 1549/1632 (94.91%)\n",
      "Train Epoch: 2 [3200/12049 (27%)]\tLoss: 0.666928\tAccuracy: 3091/3232 (95.64%)\n",
      "Train Epoch: 2 [4800/12049 (40%)]\tLoss: 0.672028\tAccuracy: 4631/4832 (95.84%)\n",
      "Train Epoch: 2 [6400/12049 (53%)]\tLoss: 0.673349\tAccuracy: 6181/6432 (96.10%)\n",
      "Train Epoch: 2 [8000/12049 (66%)]\tLoss: 0.666993\tAccuracy: 7719/8032 (96.10%)\n",
      "Train Epoch: 2 [9600/12049 (80%)]\tLoss: 0.669513\tAccuracy: 9252/9632 (96.05%)\n",
      "Train Epoch: 2 [11200/12049 (93%)]\tLoss: 0.670185\tAccuracy: 10794/11232 (96.10%)\n",
      "-------------------- training done, loss --------------------\n",
      "Trainign End at: 07/02/2020 08:50:31\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 07/02/2020 08:50:31\n",
      "Test set: Average loss: 0.0207, Accuracy: 1883/1968 (95.68%)\n",
      "Testing End at: 07/02/2020 08:50:32\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9568089430894309; Current accuracy 0.9568089430894309. Checkpointing\n",
      "Epoch End at: 07/02/2020 08:50:32\n",
      "============================================================\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"Training procedure for Quantum Computer:\")\n",
    "print(\"\\tStart at:\", time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "print(\"\\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\")\n",
    "print(\"\\tEnjoy and Good Luck!\")\n",
    "print(\"=\" * 100)\n",
    "print()\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "interest_class = [3,6]\n",
    "img_size = 4\n",
    "num_workers = 0\n",
    "batch_size = 32\n",
    "inference_batch_size = 32\n",
    "layers = [4,2]\n",
    "init_lr = 0.1\n",
    "milestones = [1]\n",
    "max_epoch = 2\n",
    "resume_path = \"\"\n",
    "training = True\n",
    "binary = False\n",
    "debug = False\n",
    "classic = False\n",
    "init_qc_lr = 0.1\n",
    "with_norm = True\n",
    "sim_range = []\n",
    "given_ang = [[1,-1,1,-1],[-1,-1]]\n",
    "train_ang = True\n",
    "save_chkp = False\n",
    "epoch_init, acc = 0, 0\n",
    "\n",
    "\n",
    "# Load data and create model\n",
    "train_loader, test_loader = load_data(interest_class,img_size,batch_size,inference_batch_size,num_workers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Net(img_size,layers,with_norm,given_ang,train_ang,training,binary,classic).to(device)\n",
    "\n",
    "# Initialize Normalization Parameters\n",
    "para_list = []\n",
    "for idx in range(len(layers)):\n",
    "    fc = getattr(model, \"fc\"+str(idx))\n",
    "    IAdj = getattr(model, \"IAdj\"+str(idx))\n",
    "    para_list.append({'params': fc.parameters(), 'lr': init_lr})\n",
    "    para_list.append({'params': IAdj.parameters(), 'lr': init_qc_lr})\n",
    "optimizer = torch.optim.Adam(para_list)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)\n",
    "\n",
    "# Training\n",
    "\n",
    "for epoch in range(epoch_init, max_epoch + 1):\n",
    "    print(\"=\" * 20, epoch, \"epoch\", \"=\" * 20)\n",
    "    print(\"Epoch Start at:\", time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "    print(\"-\" * 20, \"learning rates\", \"-\" * 20)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(param_group['lr'], end=\",\")\n",
    "    print()\n",
    "    print(\"-\" * 20, \"training\", \"-\" * 20)\n",
    "    print(\"Trainign Start at:\", time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "    train(model, optimizer, device, epoch,interest_class,criterion,train_loader)\n",
    "    print(\"Trainign End at:\", time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "    print(\"-\" * 60)\n",
    "    print()\n",
    "    print(\"-\" * 20, \"testing\", \"-\" * 20)\n",
    "    print(\"Testing Start at:\", time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "    cur_acc = test(model,device,interest_class,criterion,test_loader)\n",
    "    print(\"Testing End at:\", time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "    print(\"-\" * 60)\n",
    "    print()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    is_best = False\n",
    "    if cur_acc > acc:\n",
    "        is_best = True\n",
    "        acc = cur_acc\n",
    "    print(\"Best accuracy: {}; Current accuracy {}. Checkpointing\".format(acc, cur_acc))\n",
    "    print(\"Epoch End at:\", time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "    print(\"=\" * 60)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8213722",
   "language": "python",
   "display_name": "PyCharm (qiskit_practice)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}